{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATNKsJhsreom",
        "outputId": "5518f888-7d6a-49c2-c752-9b2c11bb60a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install bs4\n",
        "import requests\n",
        "import bs4\n",
        "import re\n",
        "from urllib.parse import urljoin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH181kbjtfW3",
        "outputId": "e57b6103-9f0e-42c1-e1f2-1197ad342086"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<title>Википедия — свободная энциклопедия</title>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start_link = 'https://ru.wikipedia.org/'\n",
        "resp = requests.get(start_link)\n",
        "soup = bs4.BeautifulSoup(resp.content, 'html.parser')\n",
        "soup.title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uU6NPHH2DOp"
      },
      "source": [
        "Функция для получения всех ссылок с переданной страницы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4JTeUT6jtjT-"
      },
      "outputs": [],
      "source": [
        "def extract_all_links(soup, filters, derived_links, start_link, max_width=10):\n",
        "    page_links = set()\n",
        "    for link in soup.find_all(\"a\", href=filters):\n",
        "        link_addr = link.get(\"href\")\n",
        "        if link_addr:\n",
        "            # дополнение относительной ссылки до полной:\n",
        "            link_addr = urljoin(start_link, link_addr)\n",
        "            # исключение уже имеющихся ссылок, чтобы не получить дерево с \"замыканиями\":\n",
        "            if link_addr not in derived_links:\n",
        "                page_links.add(link_addr)\n",
        "                if len(page_links) >= max_width:\n",
        "                    return page_links\n",
        "    return page_links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDQXdmx62oB1"
      },
      "source": [
        "Функция для рекурсивного создания дерева ссылок"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EFrJN4wq119D"
      },
      "outputs": [],
      "source": [
        "def extract_tree_of_links(\n",
        "    page_soup, filters, derived_links, start_link, max_width=10, max_depth=2\n",
        "):\n",
        "    tree_of_links = {}\n",
        "    if max_depth > 0:\n",
        "        extracted_links = extract_all_links(page_soup, filters, derived_links, start_link, max_width=max_width)\n",
        "        # множество уже полученных ссылок, чтобы не получить повторов:\n",
        "        derived_links.update(extracted_links)\n",
        "        max_depth -= 1\n",
        "        for link in extracted_links:\n",
        "            resp = requests.get(link)\n",
        "            soup = bs4.BeautifulSoup(resp.content, \"html.parser\")\n",
        "            links, _ = extract_tree_of_links(\n",
        "                soup,\n",
        "                filters,\n",
        "                derived_links,\n",
        "                link,\n",
        "                max_width=max_width,\n",
        "                max_depth=max_depth,\n",
        "            )\n",
        "            tree_of_links[link] = links\n",
        "\n",
        "    return tree_of_links, derived_links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blDeDuws3Y9J"
      },
      "source": [
        "Получение дерева ссылок с фильтрацией только тех ссылок, которые содержат 4 подряд цифры (перечисление дат текущего дня из стартовой страницы википедии)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CHzmCS2grYT6"
      },
      "outputs": [],
      "source": [
        "# фильтр ссылок с четырехзначным числом, как перечисление дат из раздела \"В этот день\"\n",
        "filters = re.compile(r\"/\\d{4}\")\n",
        "# рекурсивно получаем дерево ссылок с ограничением ширины и глубины\n",
        "tree, derived_links = extract_tree_of_links(\n",
        "    soup, filters, set(), start_link, max_width=3, max_depth=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmSD2GJF3qVO"
      },
      "source": [
        "Функция для получения из дерева ссылок конечных листовых элементов, в виде генератора"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ag5ePt1Mrnbe"
      },
      "outputs": [],
      "source": [
        "# функция-генератор \"листьев\" из дерева ссылок - возвращает по одному \"листу\" при каждом вызове\n",
        "def find_leaf(tree):\n",
        "    for node in tree.items():\n",
        "        if len(node[1]) == 0:\n",
        "            yield node[0]\n",
        "        else:\n",
        "            yield from find_leaf(node[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAX-GUC23zzX"
      },
      "source": [
        "Обход полученного дерева ссылок, и сохранение в CSV-файл первой попавшейся картинки из листовых элементов дерева, или текста страницы, если там нет картинки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eFIy2mwOtuij"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import base64\n",
        "\n",
        "# обход дерева ссылок, и получение информации для \"листьев\"\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"}\n",
        "filters = re.compile(r\".[jJ][pP][eE]?[gG]\")\n",
        "\n",
        "# создание файла CSV\n",
        "with open(\"content.csv\", \"w\", newline=\"\") as csvfile:\n",
        "    fieldnames = [\"link_addr\", \"type\", \"base64_content\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=\"\\t\")\n",
        "    writer.writeheader()\n",
        "\n",
        "for leaf in find_leaf(tree):\n",
        "    resp = requests.get(leaf)\n",
        "    soup = bs4.BeautifulSoup(resp.content, \"html.parser\")\n",
        "    obj = None\n",
        "    for img in soup.find_all(\"img\", src=filters):\n",
        "        # дополнение относительной ссылки до полной:\n",
        "        img_addr = urljoin(leaf, img.get(\"src\"))\n",
        "        img_resp = requests.get(img_addr, headers=headers)\n",
        "        if img_resp.status_code == requests.codes.ok:\n",
        "            obj = base64.b64encode(img_resp.content)\n",
        "            type = \"img\"\n",
        "            break  # поиск только одного первого изображения\n",
        "\n",
        "    # если нет ни одной картинки, то сохраним текст страницы\n",
        "    if obj is None:\n",
        "        obj = base64.b64encode(soup.text.encode(\"utf-8\"))\n",
        "        type = \"text_utf-8\"\n",
        "\n",
        "    # запись в CSV\n",
        "    with open(\"content.csv\", \"a\", newline=\"\") as csvfile:\n",
        "        fieldnames = [\"link_addr\", \"type\", \"base64_content\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=\"\\t\")\n",
        "        writer.writerow({\"link_addr\": leaf, \"type\": type, \"base64_content\": obj})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKUGU7V-4WNG"
      },
      "source": [
        "В результате получается файл CSV с тремя полями:\n",
        "* Адрес\n",
        "* Тип содержимого (img или text_utf-8)\n",
        "* Содержимое в кодировке Base64\n",
        "\n",
        "Как разделитель используется символ табуляции\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
